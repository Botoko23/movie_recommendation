{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trihoang/Desktop/movie_recommender/movienv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Access variables\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "huggingface_key = os.getenv(\"HUGGINGFACE_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('/Users/trihoang/Downloads/TMDB_all_movies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = pd.read_csv('/Users/trihoang/Downloads/TMDB_tv_dataset_v3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1066506, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#full movies\n",
    "movies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168639, 29)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#full_series\n",
    "series.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_cols = ['title', 'original_title','release_date', 'runtime', 'genres', 'overview', 'poster_path', 'imdb_rating', 'imdb_votes']\n",
    "\n",
    "movies = movies[(movies[used_cols].isna().any(axis=1)==False)][used_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['overview_word_num'] = movies['overview'].str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_movies = movies[movies['overview_word_num'].between(20, 250, 'both')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(267669, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_movies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_series_cols = ['name', 'original_name', 'first_air_date', 'number_of_seasons', 'number_of_episodes', 'genres', 'overview', 'backdrop_path', 'vote_average', 'vote_count']\n",
    "\n",
    "series = series[(series[used_series_cols].isna().any(axis=1)==False)][used_series_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "series.columns = ['title', 'original_title', 'first_air_date', 'number_of_seasons', 'number_of_episodes', 'genres', 'overview', 'poster_path', 'vote_average', 'vote_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "series['overview_word_num'] = series['overview'].str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_series = series[series['overview_word_num'].between(20, 250, 'both')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_df = pd.concat([filtered_movies, filtered_series])\n",
    "sampled_df = pd.concat([\n",
    "    filtered_series.sample(frac=0.13, random_state=41),\n",
    "    filtered_movies[filtered_movies['overview_word_num']<=50][used_cols].sample(frac=0.166, random_state=31),\n",
    "    filtered_movies[(filtered_movies['overview_word_num']>50)&(filtered_movies['overview_word_num']<=120)][used_cols].sample(frac=0.17, random_state=31),\n",
    "    filtered_movies[(filtered_movies['overview_word_num']>120)&(filtered_movies['overview_word_num']<=180)][used_cols].sample(frac=0.17, random_state=31),\n",
    "    filtered_movies[(filtered_movies['overview_word_num']>180)][used_cols].sample(frac=0.17, random_state=31)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50026, 15)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def calculate_cost(embedding_model, text_list: list[str]):\n",
    "    model_cost = {\n",
    "        'text-embedding-3-small': 0.02,\n",
    "        'text-embedding-3-large': 0.13,\n",
    "        'text-embedding-ada-002': 0.10\n",
    "    }\n",
    "    token_num  = []\n",
    "    for row in tqdm(text_list, total=len(text_list)):\n",
    "        token_num.append(num_tokens_from_string(row, \"cl100k_base\"))    \n",
    "\n",
    "    return sum(token_num) / 1_000_000 * model_cost[embedding_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50026/50026 [00:01<00:00, 28818.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07120362000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(calculate_cost('text-embedding-3-small', sampled_df['overview'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import asyncio\n",
    "import nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.AsyncOpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df['embeddings'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = sampled_df.iloc[26:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1g/q13cs_s54gz8b27r5pn3b9t00000gn/T/ipykernel_15284/2903455303.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sampled_df.drop(columns='index', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "sampled_df.reset_index(inplace=True)\n",
    "sampled_df.drop(columns='index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:47<38:50, 47.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [01:09<25:58, 32.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [02:28<42:13, 53.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [02:52<32:03, 41.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [03:27<29:38, 39.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at 5000 rows.\n",
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [03:49<24:37, 33.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [04:32<26:13, 36.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [05:17<27:27, 39.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [05:41<23:32, 34.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [06:07<21:22, 32.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at 10000 rows.\n",
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [06:31<19:07, 29.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12/50 [06:53<17:16, 27.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [07:15<15:44, 25.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14/50 [07:36<14:32, 24.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [07:59<13:54, 23.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at 15000 rows.\n",
      "15000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [08:22<13:19, 23.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17/50 [08:45<12:48, 23.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 18/50 [09:20<14:25, 27.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [10:02<16:11, 31.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 20/50 [10:28<14:57, 29.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at 20000 rows.\n",
      "20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [11:00<14:45, 30.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 22/50 [11:37<15:06, 32.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 23/50 [12:02<13:33, 30.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 24/50 [12:47<15:03, 34.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 25/50 [13:11<13:06, 31.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at 25000 rows.\n",
      "25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [13:32<11:20, 28.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 27/50 [14:16<12:41, 33.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 28/50 [14:42<11:16, 30.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 29/50 [15:15<11:00, 31.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 30/50 [15:41<09:59, 29.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at 30000 rows.\n",
      "30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [16:10<09:24, 29.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 32/50 [16:32<08:12, 27.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 33/50 [16:56<07:28, 26.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 34/50 [17:20<06:50, 25.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 35/50 [18:09<08:10, 32.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at 35000 rows.\n",
      "35000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 36/50 [18:57<08:41, 37.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 37/50 [20:08<10:15, 47.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 38/50 [20:45<08:51, 44.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 39/50 [21:12<07:08, 38.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 40/50 [24:03<13:07, 78.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at 40000 rows.\n",
      "40000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [24:28<09:23, 62.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 42/50 [25:39<08:40, 65.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 43/50 [43:47<43:22, 371.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 44/50 [44:30<27:20, 273.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 45/50 [44:55<16:33, 198.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at 45000 rows.\n",
      "45000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 46/50 [45:21<09:47, 146.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 47/50 [45:45<05:29, 109.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 48/50 [46:11<02:49, 84.85s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [46:35<01:06, 66.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [47:01<00:00, 56.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at 50000 rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to openai_embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "# model distillation, eval in openai\n",
    "# maybe test the 100k version with embedding-large later\n",
    "step = 1000\n",
    "checkpoint_interval = 5000\n",
    "\n",
    "def save_checkpoint(df: pd.DataFrame, file_path=\"checkpoint.pkl\"):\n",
    "    df.to_pickle(file_path)\n",
    "\n",
    "async def get_openai_response(text):\n",
    "    embedding = await client.embeddings.create(input=[text],\n",
    "                                               model=\"text-embedding-3-small\")\n",
    "    return embedding.data[0].embedding\n",
    "\n",
    "async def process_all_prompts(texts):\n",
    "    tasks = [get_openai_response(text) for text in texts]  # Create async tasks\n",
    "    return await asyncio.gather(*tasks)  # Run all tasks at once\n",
    "\n",
    "for i in tqdm(range(0, len(sampled_df), step)):\n",
    "    print(i)\n",
    "    embeddings = asyncio.run(process_all_prompts(sampled_df[i:i+step]))\n",
    "    sampled_df.iloc[i:i+step, -1] = pd.Series(embeddings, index=sampled_df.index[i:i+step])\n",
    "    time.sleep(15)\n",
    "\n",
    "    # Save checkpoint at every interval\n",
    "    if (i + step) % checkpoint_interval == 0:\n",
    "        save_checkpoint(sampled_df, f'checkpoint_{i+step}.pkl')\n",
    "        print(f\"Checkpoint saved at {i + step} rows.\")\n",
    "\n",
    "sampled_df.to_pickle(\"openai_embeddings.pkl\")\n",
    "print(\"Results saved to openai_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df = pd.read_pickle(\"openai_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_embeddings_df = embeddings_df.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33553</th>\n",
       "      <td>[0.028374338522553444, 0.0070473141968250275, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9427</th>\n",
       "      <td>[0.012964552268385887, 0.03231743350625038, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>[0.0018504128092899919, -0.005899613257497549,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12447</th>\n",
       "      <td>[0.015956750139594078, 0.024705497547984123, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39489</th>\n",
       "      <td>[0.025965319946408272, 0.05835741385817528, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7110</th>\n",
       "      <td>[0.0397479310631752, 0.07066299021244049, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46643</th>\n",
       "      <td>[0.027213390916585922, 0.060060519725084305, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5440</th>\n",
       "      <td>[0.01369396410882473, 0.017240161076188087, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33017</th>\n",
       "      <td>[-0.000996461370959878, 0.024415280669927597, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47138</th>\n",
       "      <td>[-0.027170047163963318, 0.057654157280921936, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              embeddings\n",
       "33553  [0.028374338522553444, 0.0070473141968250275, ...\n",
       "9427   [0.012964552268385887, 0.03231743350625038, -0...\n",
       "199    [0.0018504128092899919, -0.005899613257497549,...\n",
       "12447  [0.015956750139594078, 0.024705497547984123, 0...\n",
       "39489  [0.025965319946408272, 0.05835741385817528, -0...\n",
       "...                                                  ...\n",
       "7110   [0.0397479310631752, 0.07066299021244049, -0.0...\n",
       "46643  [0.027213390916585922, 0.060060519725084305, 0...\n",
       "5440   [0.01369396410882473, 0.017240161076188087, -0...\n",
       "33017  [-0.000996461370959878, 0.024415280669927597, ...\n",
       "47138  [-0.027170047163963318, 0.057654157280921936, ...\n",
       "\n",
       "[25000 rows x 1 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_embeddings_df.iloc[:25000]['embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings_1 = np.array(shuffled_embeddings_df.iloc[:25000]['embeddings'].tolist(), dtype=np.float32)\n",
    "embeddings_2 = np.array(embeddings_df['embeddings'].tolist(), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: tensor([[0.2962, 0.1831, 0.1834,  ..., 0.3664, 0.1478, 0.2298],\n",
      "        [0.0980, 0.2334, 0.2922,  ..., 0.2832, 0.0750, 0.1842],\n",
      "        [0.1774, 0.1822, 0.1762,  ..., 0.1841, 0.2156, 0.2734],\n",
      "        ...,\n",
      "        [0.1667, 0.1929, 0.3352,  ..., 0.3547, 0.0802, 0.2822],\n",
      "        [0.1019, 0.2300, 0.3428,  ..., 0.3088, 0.0975, 0.2038],\n",
      "        [0.2708, 0.3231, 0.3365,  ..., 0.4159, 0.1464, 0.3002]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers.util import pytorch_cos_sim\n",
    "import torch\n",
    "\n",
    "\n",
    "# Convert NumPy embeddings to PyTorch tensors\n",
    "embedding_tensor1 = torch.tensor(embeddings_1)\n",
    "embedding_tensor2 = torch.tensor(embeddings_2)\n",
    "\n",
    "# Compute cosine similarity\n",
    "similarity_score = pytorch_cos_sim(embedding_tensor1, embedding_tensor2)\n",
    "print(\"Cosine Similarity:\", similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25000, 50000])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_similarity_indices = torch.argsort(similarity_score,  dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:10<00:00, 2497.51it/s]\n"
     ]
    }
   ],
   "source": [
    "negative_pair_index_list = []\n",
    "\n",
    "for i in tqdm(range(len(similarity_score))):\n",
    "\n",
    "    # Start with the smallest similarity index for the current row\n",
    "    neg_j = torch.randint(0, 500, (1,)).item()\n",
    "\n",
    "    neg_index = int(sorted_similarity_indices[i][neg_j])\n",
    "\n",
    "    # Ensure the index is unique\n",
    "    while neg_index in negative_pair_index_list:\n",
    "        neg_index = int(sorted_similarity_indices[i][torch.randint(0, 3000, (1,)).item()])\n",
    "\n",
    "    negative_pair_index_list.append(neg_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:05<00:00, 4721.03it/s]\n"
     ]
    }
   ],
   "source": [
    "positive_pair_index_list = []\n",
    "\n",
    "for i in tqdm(range(len(similarity_score))):\n",
    "\n",
    "    pos_j = torch.randint(-30, -1, (1,)).item()\n",
    "\n",
    "    neg_index = int(sorted_similarity_indices[i][neg_j])\n",
    "    # pos_index = int(sorted_similarity_indices[i][pos_j])\n",
    "\n",
    "    while pos_index in positive_pair_index_list:\n",
    "        pos_index = int(sorted_similarity_indices[i][torch.randint(-60, -1, (1,)).item()])\n",
    "\n",
    "    positive_pair_index_list.append(pos_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 16)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_embeddings_df = shuffled_embeddings_df.iloc[:25000]\n",
    "shuffled_embeddings_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1g/q13cs_s54gz8b27r5pn3b9t00000gn/T/ipykernel_17085/524554817.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  shuffled_embeddings_df['negative_overview'] = embeddings_df['overview'].iloc[negative_pair_index_list].values\n",
      "/var/folders/1g/q13cs_s54gz8b27r5pn3b9t00000gn/T/ipykernel_17085/524554817.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  shuffled_embeddings_df['positive_overview'] = embeddings_df['overview'].iloc[positive_pair_index_list].values\n"
     ]
    }
   ],
   "source": [
    "shuffled_embeddings_df['negative_overview'] = embeddings_df['overview'].iloc[negative_pair_index_list].values\n",
    "shuffled_embeddings_df['positive_overview'] = embeddings_df['overview'].iloc[positive_pair_index_list].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overview</th>\n",
       "      <th>negative_overview</th>\n",
       "      <th>positive_overview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33553</th>\n",
       "      <td>Gabrielle has just joined a prestigious news p...</td>\n",
       "      <td>What caused Building 7 to collapse on 9/11? Dr...</td>\n",
       "      <td>A woman with a shadowed past lures a couple in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9427</th>\n",
       "      <td>Fay reconnects with her sister, Alice, as she ...</td>\n",
       "      <td>«Svejk v civilu» (also known as \"Svejk as a Ci...</td>\n",
       "      <td>A television star goes home to Texas for her f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>After a billionaire engineer witnesses his bes...</td>\n",
       "      <td>Hitotsu Yane no Shita is a Japanese television...</td>\n",
       "      <td>When one of his former colleagues is murdered,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12447</th>\n",
       "      <td>A large group of criminals escapes from prison...</td>\n",
       "      <td>Clare Balding rounds up every GB medal as the ...</td>\n",
       "      <td>During a routine prison work detail, convict P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39489</th>\n",
       "      <td>The sun is setting and we see Dave, an artist,...</td>\n",
       "      <td>Recording the journey of Raisa, a great Indone...</td>\n",
       "      <td>After the murder of his beloved wife, a man in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7110</th>\n",
       "      <td>Home with their newly-formed family, happy par...</td>\n",
       "      <td>Following his discharge from the Navy for hitt...</td>\n",
       "      <td>Jake and Desmond (Paranormal Exterminators) te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46643</th>\n",
       "      <td>November 1970. Several human remains appear in...</td>\n",
       "      <td>In New York City's Chinatown, an ornery, chain...</td>\n",
       "      <td>Prisoners await execution by firing squad when...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5440</th>\n",
       "      <td>Léo goes on vacation at his cousin's, in a fis...</td>\n",
       "      <td>Putting together stunning visual and performan...</td>\n",
       "      <td>On her trip back from a working holiday abroad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33017</th>\n",
       "      <td>Story set in the Middle Ages. A page and the d...</td>\n",
       "      <td>It’s summer again, and everyone’s favorite Jun...</td>\n",
       "      <td>A teenage girl in Medieval England navigates l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47138</th>\n",
       "      <td>When a group of unsuspecting teenagers get tog...</td>\n",
       "      <td>Ada Harris, a London charwoman in the 1950's, ...</td>\n",
       "      <td>Six thirty-somethings try to spice up their st...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                overview  \\\n",
       "33553  Gabrielle has just joined a prestigious news p...   \n",
       "9427   Fay reconnects with her sister, Alice, as she ...   \n",
       "199    After a billionaire engineer witnesses his bes...   \n",
       "12447  A large group of criminals escapes from prison...   \n",
       "39489  The sun is setting and we see Dave, an artist,...   \n",
       "...                                                  ...   \n",
       "7110   Home with their newly-formed family, happy par...   \n",
       "46643  November 1970. Several human remains appear in...   \n",
       "5440   Léo goes on vacation at his cousin's, in a fis...   \n",
       "33017  Story set in the Middle Ages. A page and the d...   \n",
       "47138  When a group of unsuspecting teenagers get tog...   \n",
       "\n",
       "                                       negative_overview  \\\n",
       "33553  What caused Building 7 to collapse on 9/11? Dr...   \n",
       "9427   «Svejk v civilu» (also known as \"Svejk as a Ci...   \n",
       "199    Hitotsu Yane no Shita is a Japanese television...   \n",
       "12447  Clare Balding rounds up every GB medal as the ...   \n",
       "39489  Recording the journey of Raisa, a great Indone...   \n",
       "...                                                  ...   \n",
       "7110   Following his discharge from the Navy for hitt...   \n",
       "46643  In New York City's Chinatown, an ornery, chain...   \n",
       "5440   Putting together stunning visual and performan...   \n",
       "33017  It’s summer again, and everyone’s favorite Jun...   \n",
       "47138  Ada Harris, a London charwoman in the 1950's, ...   \n",
       "\n",
       "                                       positive_overview  \n",
       "33553  A woman with a shadowed past lures a couple in...  \n",
       "9427   A television star goes home to Texas for her f...  \n",
       "199    When one of his former colleagues is murdered,...  \n",
       "12447  During a routine prison work detail, convict P...  \n",
       "39489  After the murder of his beloved wife, a man in...  \n",
       "...                                                  ...  \n",
       "7110   Jake and Desmond (Paranormal Exterminators) te...  \n",
       "46643  Prisoners await execution by firing squad when...  \n",
       "5440   On her trip back from a working holiday abroad...  \n",
       "33017  A teenage girl in Medieval England navigates l...  \n",
       "47138  Six thirty-somethings try to spice up their st...  \n",
       "\n",
       "[25000 rows x 3 columns]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_embeddings_df[['overview', 'negative_overview', 'positive_overview']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset\n",
    "df = shuffled_embeddings_df[['overview', 'negative_overview', 'positive_overview']].sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split into train, validation, and test sets (e.g., 80% train, 10% validation, 10% test)\n",
    "train_frac = 0.8\n",
    "valid_frac = 0.1\n",
    "test_frac = 0.1\n",
    "\n",
    "# define train and validation size\n",
    "train_size = int(train_frac * len(df))\n",
    "valid_size = int(valid_frac * len(df))\n",
    "\n",
    "# create train, validation, and test datasets\n",
    "df_train = df[:train_size]\n",
    "df_valid = df[train_size:train_size + valid_size]\n",
    "df_test = df[train_size + valid_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(huggingface_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 20/20 [00:00<00:00, 294.38ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:07<00:00,  7.05s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:00<00:00, 152.56ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:03<00:00,  3.07s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:00<00:00, 148.17ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:03<00:00,  3.47s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/trihoang131/movie_dataset_50K/commit/060877eb323903c322d0733b3c9bfaaa698311a6', commit_message='Upload dataset', commit_description='', oid='060877eb323903c322d0733b3c9bfaaa698311a6', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/trihoang131/movie_dataset_50K', endpoint='https://huggingface.co', repo_type='dataset', repo_id='trihoang131/movie_dataset_50K'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "train_ds = Dataset.from_pandas(df_train)\n",
    "valid_ds = Dataset.from_pandas(df_valid)\n",
    "test_ds = Dataset.from_pandas(df_test)\n",
    "\n",
    "# Combine into a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_ds,\n",
    "    'validation': valid_ds,\n",
    "    'test': test_ds\n",
    "})\n",
    "\n",
    "dataset_dict.push_to_hub(\"trihoang131/movie_dataset_50K\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "movienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
